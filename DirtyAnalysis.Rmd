---
title: "Quick and Dirty Analysis of Dataset"
output: html_notebook
---

The goal of this notebook is to perform a quick analysis of the data to see if our hypothesis is explorable or not. We begin by importing necessary libraries and importing data. Our data is from [data.gov](https://data.gov.sg/dataset/resale-flat-prices).

```{r}
library(tidyverse)
library(lubridate)
library(ggplot2)

prices <- read_csv('csv/final.csv')
```

Let's see what towns are captured in our dataset.
```{r}
unique(prices$town)
```

We now need to decide on a few towns we wish to analyse over the years. We decided to use the following towns as *"control"* towns:

* Marine Parade
* Hougang
* Woodlands

We chose these due to them being quite far from the Downtown line. These locations also haven't had any new MRT stations nearby in a very long time, which should reduce the possibility of unintended price increases.

We chose the following towns to be our *"experimental"* towns:

* Tampines
* 

These tend to have quite a number of HDB blocks around them, which is what our dataset contains. We now need to extract the relevant components of our dataset and plot facet plots of them. For the purpose of our analysis, we use 4-room flats as they are the most commonly purchased/sold units.

```{r}
towns <- prices %>%
  filter(town == "MARINE PARADE" | town == "HOUGANG" | town == "WOODLANDS" | town == "TAMPINES") %>%
  filter(flat_type == "4 ROOM") %>%
  mutate(year = substr(month, 1, 4)) %>%
  mutate(resale_price = resale_price / 1000)

ggplot(towns, aes(x=year, y=resale_price)) +
  geom_bar(stat="summary", fun.y = "median") +
  facet_grid(town ~ ., scales="free_y") +
  theme(strip.text.y = element_text(angle = 0), axis.text.x = element_text(angle = 60, hjust = 1)) +
  ylab("Price in Thousands") + xlab("Year") + 
  ggtitle("Median House Prices Over the Years") +
  guides(fill=FALSE)
```

As we can see, both our control and our experimental towns have the same trends. Hence, we might need to increase the granularity of our analysis. We therefore need to geocode our data so we can properly restrict them to smaller zones.

```{r}
toGeocode <- prices %>%
  filter(flat_type == "4 ROOM") %>%
  mutate(year = substr(month, 1, 4)) %>%
  mutate(resale_price = resale_price / 1000) %>%
  mutate(location = paste0(block, " ", street_name))
  

toGeocode.unique <- toGeocode %>%
  group_by(location) %>%
  summarise(count = n())
#write_csv(toGeocode.unique, "csv/toGeocode.csv")
geocodedLocations <- read_csv("csv/geocoded.csv")
geocodedPrices <- left_join(geocodedLocations, toGeocode, by = c("location" = "location")) %>%
  filter(lat != "unknown") %>%
  mutate(lat = as.numeric(lat)) %>%
  mutate(lon = as.numeric(long))
```

Next we load in the KML of Singapore's subzones so we can perform a much finer-grained analysis of the data.

```{r}
library(sf)
buffers <- st_read("shp/Downtown Line Stations Buffer.shp")
priceShapes <- st_as_sf(geocodedPrices, coords = c("lon", "lat"))
#plot(buffers)
#plot(priceShapes, add = T)
intersection = intersect(priceShapes, buffers)
#coordinates(geocodedPrices) <- ~as.numeric(long)+as.numeric(lat)
#toGeocode %over% subzones
```

